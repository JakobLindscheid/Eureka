Importing module 'gym_38' (/home/vandriel/Documents/GitHub/Eureka/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)
Setting GYM_USD_PLUG_INFO_PATH to /home/vandriel/Documents/GitHub/Eureka/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 2.0.0+cu117
Device count 1
/home/vandriel/Documents/GitHub/Eureka/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /home/vandriel/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /home/vandriel/.cache/torch_extensions/py38_cu117/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module gymtorch...
2024-10-19 22:33:10,178 - INFO - logger - logger initialized
<unknown>:3: DeprecationWarning: invalid escape sequence \*
Error: FBX library failed to load - importing FBX data will not succeed. Message: No module named 'fbx'
FBX tools must be installed from https://help.autodesk.com/view/FBX/2020/ENU/?guid=FBX_Developer_Help_scripting_with_python_fbx_installing_python_fbx_html
/home/vandriel/anaconda3/envs/eureka/lib/python3.8/site-packages/matplotlib/__init__.py:169: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(module.__version__) < minver:
/home/vandriel/anaconda3/envs/eureka/lib/python3.8/site-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
ROOT_DIR=/home/vandriel/Documents/GitHub/Eureka/isaacgymenvs/isaacgymenvs
/home/vandriel/anaconda3/envs/eureka/lib/python3.8/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/vandriel/anaconda3/envs/eureka/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, "__version__") or LooseVersion(
task_name: HumanoidGPT
experiment: 
env_path: 
num_envs: 
seed: 42
torch_deterministic: False
max_iterations: 2000
physics_engine: physx
pipeline: gpu
sim_device: cuda:0
rl_device: cuda:0
graphics_device_id: 0
num_threads: 4
solver_type: 1
num_subscenes: 4
test: False
checkpoint: 
sigma: 
multi_gpu: False
wandb_activate: False
wandb_group: 
wandb_name: HumanoidGPT
wandb_entity: koza
wandb_project: eureka
wandb_tags: []
wandb_logcode_dir: 
capture_video: False
capture_video_freq: 5000
capture_video_len: 200
force_render: False
headless: True
task: 
    env: 
        actionsCost: 0.01
        angularVelocityScale: 0.25
        asset: 
            assetFileName: mjcf/nv_humanoid.xml
        clipActions: 1.0
        contactForceScale: 0.01
        deathCost: -1.0
        dofVelocityScale: 0.1
        enableCameraSensors: False
        enableDebugVis: False
        energyCost: 0.05
        envSpacing: 5
        env_name: humanoidGPT
        episodeLength: 1000
        headingWeight: 0.5
        jointsAtLimitCost: 0.25
        numEnvs: 4096
        plane: 
            dynamicFriction: 1.0
            restitution: 0.0
            staticFriction: 1.0
        powerScale: 1.0
        terminationHeight: 0.45
        upWeight: 0.1
    name: HumanoidGPT
    physics_engine: physx
    sim: 
        dt: 0.0166
        gravity: [1.7, 0.0, -9.66]
        physx: 
            bounce_threshold_velocity: 0.2
            contact_collection: 0
            contact_offset: 0.02
            default_buffer_size_multiplier: 5.0
            max_depenetration_velocity: 10.0
            max_gpu_contact_pairs: 8388608
            num_position_iterations: 4
            num_subscenes: 4
            num_threads: 4
            num_velocity_iterations: 0
            rest_offset: 0.0
            solver_type: 1
            use_gpu: True
        substeps: 2
        up_axis: z
        use_gpu_pipeline: True
    task: 
        randomization_params: 
            actions: 
                distribution: gaussian
                operation: additive
                range: [0.0, 0.02]
            actor_params: 
                humanoid: 
                    color: True
                    dof_properties: 
                        damping: 
                            distribution: uniform
                            operation: scaling
                            range: [0.5, 1.5]
                            schedule: linear
                            schedule_steps: 3000
                        lower: 
                            distribution: gaussian
                            operation: additive
                            range: [0, 0.01]
                            schedule: linear
                            schedule_steps: 3000
                        stiffness: 
                            distribution: uniform
                            operation: scaling
                            range: [0.5, 1.5]
                            schedule: linear
                            schedule_steps: 3000
                        upper: 
                            distribution: gaussian
                            operation: additive
                            range: [0, 0.01]
                            schedule: linear
                            schedule_steps: 3000
                    rigid_body_properties: 
                        mass: 
                            distribution: uniform
                            operation: scaling
                            range: [0.5, 1.5]
                            schedule: linear
                            schedule_steps: 3000
                            setup_only: True
                    rigid_shape_properties: 
                        friction: 
                            distribution: uniform
                            num_buckets: 500
                            operation: scaling
                            range: [0.7, 1.3]
                            schedule: linear
                            schedule_steps: 3000
                        restitution: 
                            distribution: uniform
                            operation: scaling
                            range: [0.0, 0.7]
                            schedule: linear
                            schedule_steps: 3000
            frequency: 600
            observations: 
                distribution: gaussian
                operation: additive
                range: [0, 0.002]
            sim_params: 
                gravity: 
                    distribution: gaussian
                    operation: additive
                    range: [0, 0.4]
                    schedule: linear
                    schedule_steps: 3000
        randomize: True
train: 
    params: 
        algo: 
            name: a2c_continuous
        config: 
            bounds_loss_coef: 0.0001
            clip_value: True
            critic_coef: 4
            e_clip: 0.2
            entropy_coef: 0.0
            env_name: rlgpu
            full_experiment_name: HumanoidGPT
            gamma: 0.99
            grad_norm: 1.0
            horizon_length: 32
            kl_threshold: 0.008
            learning_rate: 0.0005
            lr_schedule: adaptive
            max_epochs: 2000
            mini_epochs: 5
            minibatch_size: 32768
            mixed_precision: True
            multi_gpu: False
            name: HumanoidGPT
            normalize_advantage: True
            normalize_input: True
            normalize_value: True
            num_actors: 4096
            ppo: True
            print_stats: True
            reward_shaper: 
                scale_value: 0.01
            save_best_after: 200
            save_frequency: 100
            score_to_win: 20000
            seq_len: 4
            tau: 0.95
            truncate_grads: True
            value_bootstrap: True
        load_checkpoint: False
        load_path: 
        model: 
            name: continuous_a2c_logstd
        network: 
            mlp: 
                activation: elu
                d2rl: False
                initializer: 
                    name: default
                regularizer: 
                    name: None
                units: [400, 200, 100]
            name: actor_critic
            separate: False
            space: 
                continuous: 
                    fixed_sigma: True
                    mu_activation: None
                    mu_init: 
                        name: default
                    sigma_activation: None
                    sigma_init: 
                        name: const_initializer
                        val: 0
        seed: 42
Setting seed: 42
Network Directory: /home/vandriel/Documents/GitHub/Eureka/eureka/outputs/eureka/2024-10-19_20-11-28/policy-2024-10-19_22-33-10/runs/HumanoidGPT-2024-10-19_22-33-10/nn
Tensorboard Directory: /home/vandriel/Documents/GitHub/Eureka/eureka/outputs/eureka/2024-10-19_20-11-28/policy-2024-10-19_22-33-10/runs/HumanoidGPT-2024-10-19_22-33-10/summaries
self.seed = 42
Started to train
/home/vandriel/anaconda3/envs/eureka/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
[Warning] [carb.gym.plugin] useGpu is set, forcing single scene (0 subscenes)
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
/home/vandriel/Documents/GitHub/Eureka/isaacgymenvs/isaacgymenvs/tasks/humanoidgpt.py:126: DeprecationWarning: an integer is required (got type isaacgym._bindings.linux-x86_64.gym_38.DofDriveMode).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  asset_options.default_dof_drive_mode = gymapi.DOF_MODE_NONE
Box(-1.0, 1.0, (21,), float32) Box(-inf, inf, (108,), float32)
current training device: cuda:0
build mlp: 108
RunningMeanStd:  (1,)
RunningMeanStd:  (108,)
Error executing job with overrides: ['task=HumanoidGPT', 'wandb_activate=False', 'wandb_entity=koza', 'wandb_project=eureka', 'headless=True', 'capture_video=False', 'force_render=False', 'max_iterations=2000']
Traceback (most recent call last):
  File "/home/vandriel/Documents/GitHub/Eureka/eureka/../isaacgymenvs/isaacgymenvs/train.py", line 207, in launch_rlg_hydra
    statistics = runner.run({
  File "/home/vandriel/Documents/GitHub/Eureka/rl_games/rl_games/torch_runner.py", line 124, in run
    self.run_train(args)
  File "/home/vandriel/Documents/GitHub/Eureka/rl_games/rl_games/torch_runner.py", line 101, in run_train
    self.agent.train()
  File "/home/vandriel/Documents/GitHub/Eureka/rl_games/rl_games/common/a2c_common.py", line 1251, in train
    step_time, play_time, update_time, sum_time, a_losses, c_losses, b_losses, entropies, kls, last_lr, lr_mul = self.train_epoch()
  File "/home/vandriel/Documents/GitHub/Eureka/rl_games/rl_games/common/a2c_common.py", line 1115, in train_epoch
    batch_dict = self.play_steps()
  File "/home/vandriel/Documents/GitHub/Eureka/rl_games/rl_games/common/a2c_common.py", line 686, in play_steps
    self.obs, rewards, self.dones, infos = self.env_step(res_dict['actions'])
  File "/home/vandriel/Documents/GitHub/Eureka/rl_games/rl_games/common/a2c_common.py", line 504, in env_step
    obs, rewards, dones, infos = self.vec_env.step(actions)
  File "/home/vandriel/Documents/GitHub/Eureka/isaacgymenvs/isaacgymenvs/utils/rlgames_utils.py", line 256, in step
    return  self.env.step(actions)
  File "/home/vandriel/Documents/GitHub/Eureka/isaacgymenvs/isaacgymenvs/tasks/base/vec_task.py", line 355, in step
    self.post_physics_step()
  File "/home/vandriel/Documents/GitHub/Eureka/isaacgymenvs/isaacgymenvs/tasks/humanoidgpt.py", line 267, in post_physics_step
    self.compute_reward(self.actions)
  File "/home/vandriel/Documents/GitHub/Eureka/isaacgymenvs/isaacgymenvs/tasks/humanoidgpt.py", line 187, in compute_reward
    self.rew_buf[:], self.rew_dict = compute_reward(self.root_states, self.dof_force_tensor, self.angle_to_target, self.dof_vel)
AttributeError: 'HumanoidGPT' object has no attribute 'angle_to_target'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
